{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137629 chars, 81 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d chars, %d unique' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'C': 31, '!': 3, ' ': 4, '\"': 5, '%': 6, '$': 7, \"'\": 8, ')': 9, '(': 10, '*': 11, '-': 12, ',': 13, '/': 2, '.': 15, '1': 16, '0': 17, '3': 18, '2': 19, '5': 20, '4': 21, '7': 22, '6': 23, '9': 24, '8': 25, ';': 26, ':': 27, '?': 28, 'A': 29, '@': 30, '\\xc3': 1, 'B': 32, 'E': 33, 'D': 34, 'G': 35, 'F': 36, 'I': 37, 'H': 38, 'K': 39, 'J': 40, 'M': 41, 'L': 42, 'O': 43, 'N': 44, 'Q': 45, 'P': 46, 'S': 47, 'R': 48, 'U': 49, 'T': 50, 'W': 51, 'V': 52, 'Y': 53, 'X': 54, 'd': 59, 'a': 55, 'c': 56, 'b': 57, 'e': 58, '\\xa7': 14, 'g': 60, 'f': 61, 'i': 62, 'h': 63, 'k': 64, 'j': 65, 'm': 66, 'l': 67, 'o': 68, 'n': 69, 'q': 70, 'p': 71, 's': 72, 'r': 73, 'u': 74, 't': 75, 'w': 76, 'v': 77, 'y': 78, 'x': 79, 'z': 80}\n",
      "{0: '\\n', 1: '\\xc3', 2: '/', 3: '!', 4: ' ', 5: '\"', 6: '%', 7: '$', 8: \"'\", 9: ')', 10: '(', 11: '*', 12: '-', 13: ',', 14: '\\xa7', 15: '.', 16: '1', 17: '0', 18: '3', 19: '2', 20: '5', 21: '4', 22: '7', 23: '6', 24: '9', 25: '8', 26: ';', 27: ':', 28: '?', 29: 'A', 30: '@', 31: 'C', 32: 'B', 33: 'E', 34: 'D', 35: 'G', 36: 'F', 37: 'I', 38: 'H', 39: 'K', 40: 'J', 41: 'M', 42: 'L', 43: 'O', 44: 'N', 45: 'Q', 46: 'P', 47: 'S', 48: 'R', 49: 'U', 50: 'T', 51: 'W', 52: 'V', 53: 'Y', 54: 'X', 55: 'a', 56: 'c', 57: 'b', 58: 'e', 59: 'd', 60: 'g', 61: 'f', 62: 'i', 63: 'h', 64: 'k', 65: 'j', 66: 'm', 67: 'l', 68: 'o', 69: 'n', 70: 'q', 71: 'p', 72: 's', 73: 'r', 74: 'u', 75: 't', 76: 'w', 77: 'v', 78: 'y', 79: 'x', 80: 'z'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print char_to_ix\n",
    "print ix_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode char\n",
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print vector_for_char_a.ravel()\n",
    "# print vector_for_char_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossFun -> loss Function\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    inputs,targets are both list of integers.                                                                                                                                                   \n",
    "    hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "    returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "    \"\"\"\n",
    "    #store our inputs, hidden states, outputs, and probability values\n",
    "    xs, hs, ys, ps = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "\n",
    "    #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "    hs[-1] = np.copy(hprev)\n",
    "\n",
    "    #init loss as 0\n",
    "    loss = 0\n",
    "\n",
    "\n",
    "    # forward pass                                                                                                                                                                              \n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "        xs[t][inputs[t]] = 1 # one-hot encode input char number\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "\n",
    "    # backward pass: compute gradients going backwards , BPTT   \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        # output probabilities\n",
    "        # http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "        dy = np.copy(ps[t])\n",
    "        \n",
    "        #derive our first gradient\n",
    "        dy[targets[t]] -= 1 # backprop into y\n",
    "        \n",
    "        #compute output gradient -  output times hidden states transpose\n",
    "        #When we apply the transpose weight matrix,  \n",
    "        #we can think intuitively of this as moving the error backward\n",
    "        #through the network, giving us some sort of measure of the error \n",
    "        #at the output of the lth layer. \n",
    "        #output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        \n",
    "        #derivative of output bias\n",
    "        dby += dy\n",
    "        \n",
    "        #backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " YpM;\"R :! 0z0d:O\"HfF @01MKl(6'@M5U$;/,USAM4zBBMW�u'miylQb7v(M5VRp16J��soua4cN08s-1$184sP�oeR\n",
      " M.�\"Y�@n?k\n",
      "-Na�zk%\"@JAf0*8�S\n",
      "o):QJT�%('z*YXMC:zP7Bi3,T01a8\"8XRoV9D0qP'ttzO�ErWw$Wd8Pw/;Kon2as4T*pOiIG@pV�. \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"                                                                                                                                                                                         \n",
    "    sample a sequence of integers from the model                                                                                                                                                \n",
    "    h is memory state, seed_ix is seed letter for first time step   \n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    #create vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for our seed char\n",
    "    x[seed_ix] = 1\n",
    "    \n",
    "    #list to store generated chars\n",
    "    ixes = []\n",
    "    \n",
    "    #for as many characters as we want to generate\n",
    "    for t in xrange(n):\n",
    "        #a hidden state at a given time step is a function \n",
    "        #of the input at the same time step modified by a weight matrix \n",
    "        #added to the hidden state of the previous time step \n",
    "        #multiplied by its own hidden state to hidden state matrix.\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        \n",
    "        #compute output (unnormalised)\n",
    "        y = np.dot(Why, h) + by\n",
    "        \n",
    "        ## probabilities for next chars\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        \n",
    "        #pick one with the highest probability \n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        #create a vector\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        #customize it for the predicted char\n",
    "        x[ix] = 1\n",
    "        \n",
    "        #add it to the list\n",
    "        ixes.append(ix)\n",
    "\n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "#     print '----\\n %s \\n----' % txt\n",
    "    \n",
    "    \n",
    "# do sample test\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "\n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [43, 69, 58, 4, 66, 68, 73, 69, 62, 69, 60, 13, 4, 76, 63, 58, 69, 4, 35, 73, 58, 60, 68, 73, 4]\n",
      "targets [69, 58, 4, 66, 68, 73, 69, 62, 69, 60, 13, 4, 76, 63, 58, 69, 4, 35, 73, 58, 60, 68, 73, 4, 47]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print \"inputs\", inputs\n",
    "\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print \"targets\", targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.814562\n",
      "----\n",
      " o teepter hitttom ofacxdfe srpoy nave sunite owenne fselith tim-, s,on up, ande iiy, hisre torfiungskk fnd htciy ct chinvin nirn the cthrf, wid inpe tinfrs me cad tersget, bsepre tat hirt euy fnme o t \n",
      "----\n",
      "iter 1000, loss: 80.147450\n",
      "----\n",
      "  hitono ha thats to thit thees, \"Lass un hlin that to thor -lre wey an onde do ktod dus the ferpthasiwnawe!iind buhspulf the do the cel gasl. ther heom,  couthiff and the fat to med tome  he fede foil \n",
      "----\n",
      "iter 2000, loss: 63.557886\n",
      "----\n",
      " urkather sgre futk he mo he; sersewh whem. Suchtorser -n heeof coind. He whios mishing on shack ound would ashed his whihas slremime wher woulk croned orlroued ardractsgheich hishen outefe of to hor's \n",
      "----\n",
      "iter 3000, loss: 55.655830\n",
      "----\n",
      " trinaughe; shevyo;et more;ter erre an hare spomenyor crerlimes bengun erkhing. Br to wobminke coon bis peston domous ouretreshoul; erely inly gocck glle sbagsund; wathind firered creusd af toosathered \n",
      "----\n",
      "iter 4000, loss: 51.796909\n",
      "----\n",
      " hey ie in thet witl it asimack thele peoun ebe mkex ind, with oumsy as lere would was an to shiwiwsar they it their ad with ous on chalu his nowing the ouid, beanimeryoane nit kiebuin lishion doon the \n",
      "----\n",
      "iter 5000, loss: 54.128341\n",
      "----\n",
      "  pry hap in the set in proad  11 weout forcy Euyhit. the juta\n",
      "1 any. YEaing the Prokter horest\"-uck I) 1 ted juf to tor1 1ou1as on chapperee to 1hape bussed ropior be 1ed stoor. ut thtthadlade - 1Eper \n",
      "----\n",
      "iter 6000, loss: 56.958061\n",
      "----\n",
      "  nAthed pallen- and and as pet, hrad hitgob of jurelnils bre if wandid theg bedether leon heuntand are hof ing be pere thon a gelr ar wang cten cany po heco gob ath legheed wher the fanceda des ackgen \n",
      "----\n",
      "iter 7000, loss: 53.966783\n",
      "----\n",
      " y the cuhangor. He atar than doot sas he fave fasta fore Gregor dolder ande aif toun sre andedo woune sann herk in hal thomeror, heavow Gregoird has. Hmo fous hor four thrpid the wang was re thanke me \n",
      "----\n",
      "iter 8000, loss: 51.320694\n",
      "----\n",
      " amele verper she had patt. I\" cowe. Lle fis p, ab had flat t lien on food the toly had wisty all tparas love coled co mait and it hadd gottreve conce vetilkor No en meelaover crf pleafnoldt bit; the f \n",
      "----\n",
      "iter 9000, loss: 49.846187\n",
      "----\n",
      " r le seat;. Th utseirhingtilrtose, quning- and, anveallis andleded, has waint moomping bacer and thogect in the toucforeesrester. Hes let pouve wasty tan tooke bety bespasut - want tooug. But cowing o \n",
      "----\n",
      "iter 10000, loss: 48.921042\n",
      "----\n",
      " n bed the githere alw\" \"Gregoroctisalr .\n",
      "\n",
      "Gregor's mby afvere so, bomes'nd recn seaking. Tt wackey.y'n fay? I's weew wad ot loother. In llost buther exser' dome that if this wing this at sime paar's ' \n",
      "----\n",
      "iter 11000, loss: 54.919406\n",
      "----\n",
      " forpas loye patiping to yow duyie8 workatlersonctod. witm thinsony Soutey: Ale iny anipadcehticiss\n",
      "m\n",
      "wTicintichaik\n",
      " puppithio foicheg and /E of yhas ch aipactow cove Grtelle the Pratmceene-t toreien\n",
      "\n",
      " \n",
      "----\n",
      "iter 12000, loss: 51.633304\n",
      "----\n",
      " wol wiovl, tres's whsery he s orse-s for plite; the evor how yould cere se the tory thon; seppye? we wat lo projlit fould the cere he vere tht ant thing wive lave plom he gut ept waspegot. He of wotio \n",
      "----\n",
      "iter 13000, loss: 48.676048\n",
      "----\n",
      " se he lroumd, as had epting any oney thar boe weie ent the to was mengt ant to woveed bomethem at her phim. Ghem, ken toyt gess bep so the anon. Fouty kistraly his anoughalyy overy that hisp, the wing \n",
      "----\n",
      "iter 14000, loss: 46.889906\n",
      "----\n",
      " bucking to waths him wathinlee fids- ther, pras unghite (ain whige paonedy wot flythinghing out hay neat; aly, hill pacht turking, wheysy thr could wha bud aw the ked as biven, veur,, ho how exugherse \n",
      "----\n",
      "iter 15000, loss: 46.178286\n",
      "----\n",
      " ed beemed bene wore mocl tayintaistast at ay him rathind blay his mothercemet t olin and to the clowchand the ore stolg the, averarened thing. - lit moinher exst, anct frominenunr, plin. \"Magren in si \n",
      "----\n",
      "iter 16000, loss: 48.682938\n",
      "----\n",
      " outesting the gesticew therals thild oowideru on this on bet, withourisg. - tise, exsedenas nott the werkoy:\n",
      "\n",
      "workiveting whe forsathiftentt\n",
      "\n",
      "aresed inly the wore onding this stion. Ufactretsay comed  \n",
      "----\n",
      "iter 17000, loss: 50.403096\n",
      "----\n",
      "  on and rust\n",
      "back, thoug the fork latt thinkmod this be ded hume widl ame the pbemion looke; ind aine towgo forking, atly oin monelling morked quite ack of ining, the dord ow their pagheecher,orted be \n",
      "----\n",
      "iter 18000, loss: 47.728676\n",
      "----\n",
      " \n",
      " Gregor'stat fou do Gregor'geblsing be theighing pleal' have with as alf poon anechen tisto provely at onbee the maslu the carked sao knost on thoughoute bedilksury not it lable blbad hyiady hwa dot  \n",
      "----\n",
      "iter 19000, loss: 45.642011\n",
      "----\n",
      "  I whit aster clo an, alther he for the doaking be sperist, planded Chas fromund of ress Grefor nom; of himselfyee moom, the room peppaly voreay watchelf thiss roomsong dighingvenfiom e prosterent ove \n",
      "----\n",
      "iter 20000, loss: 44.957085\n",
      "----\n",
      "  ser had wand that a\n",
      "thery poor buthippaiven in that bet bectowged thot apditherelppind lesiodaelf the conten of - fere. Whis itmer an, the corder she dedlellis-veired ; the hreaped ap. Gregor tout qu \n",
      "----\n",
      "iter 21000, loss: 44.685645\n",
      "----\n",
      " ows room ald it. The pusteus horessy at ino hadly ald ent expireasing juimist more dome not that biuncion working ot sowed his lidved hould apie of the ofmep.\"\n",
      "\"thoughingag in daras dole to knoking as \n",
      "----\n",
      "iter 22000, loss: 49.901436\n",
      "----\n",
      " friance quer.  Un- Projeroberde moleftictlrsidered Greplimes be hor he collied the at thears, that\n",
      "ir ale and sid. Sere on of bof quice and exter or of anl th. 50Tiritenf. WKains Prang blepess thod he \n",
      "----\n",
      "iter 23000, loss: 47.782412\n",
      "----\n",
      " inave thered relly, the er cim! him thall ht urst pleerted thent. But his was the hes handne iicelbod to coed sengeme efto liacl his sither nom his of ceigg be that theledy. Jeding coplrenge wior want \n",
      "----\n",
      "iter 24000, loss: 45.441973\n",
      "----\n",
      "  the for quid robeen but hould and ringer a meck in dikiod mones as sooked muck if him not ert she hen ther his a why comify the freme st astray cheaven and but lot cleess robincerys who fis even whom \n",
      "----\n",
      "iter 25000, loss: 43.898827\n",
      "----\n",
      " s siuv; eve, wabe aited aod ain wull had loke ster him a fare Gregor on to conf thoulf in only tt with, kexplet\n",
      "bot forss thas even and  hon - love wishauna to he cas wat of ilis to bow agly Huaraar a \n",
      "----\n",
      "iter 26000, loss: 43.539159\n",
      "----\n",
      " y a lither her herrence the foretrer ther share. Greg. \"Prived warly souding to once egel wout righinar. She op tere to thet ass her momed gong gay remert sound Gregor's corithout ke comerer mis lore  \n",
      "----\n",
      "iter 27000, loss: 45.754549\n",
      "----\n",
      " ovever cked erstion.\n",
      "\n",
      "Pretiveve the kfrein if wate withast it the Fouly,. heppets, and cea linders.\n",
      "\n",
      "Projestirg-t turself withwertainingibencangense teede juste this astaytech ibert weir arsulce an di \n",
      "----\n",
      "iter 28000, loss: 47.702880\n",
      "----\n",
      "  Sole comenber' swooked sed end it bude ind he nok lo alk out his tilely eflem ould ha piech ofceraut forming for it by be in wh. He was acswst mititer bytoade. Arftided she lone bevers a soung that s \n",
      "----\n",
      "iter 29000, loss: 45.479777\n",
      "----\n",
      "  bedar thinginelochout kfy neince fawhat erist been thaw a oul fut lition, see cossust asthes with joce sollintansing to thes of dad to Gregor's lid in of the door himpuld brow his fible terfoled the  \n",
      "----\n",
      "iter 30000, loss: 43.596444\n",
      "----\n",
      " feareself core stuunde, eftred a where of have this nos to as the wis explalyentilithing now feechatecidy his she quecas, af far to himped the doon bely, thay Proe loftid.  oule the claen sly he campl \n",
      "----\n",
      "iter 31000, loss: 43.105188\n",
      "----\n",
      " isturd plusentoon. And had comery with ditherf lo budyse, ant and wall upter bufe sleare ant with mord mack pened the floor hi- sing; the door ancher sinelyeriaitay wad a out they head to her event ta \n",
      "----\n",
      "iter 32000, loss: 42.873585\n",
      "----\n",
      " nd out they bedints allis wiis, but it a the was reaine pllete boot: whe inacksall and waid raate as slead for she had layoof.\"\n",
      "No\"d\". whik rolleouf? Mrejur but from rrline anninink. Him libely the ms \n",
      "----\n",
      "iter 33000, loss: 47.650627\n",
      "----\n",
      " ricwatelefoem. ASD SUNfert-bron\n",
      "thare Pround tire the Projuctrf\n",
      "\n",
      "omoif locked Projeclesiar\n",
      "  fotsed a now the Pat foll bocy\n",
      "1 F1 horxalectoG Prce dident yout bucint bregeraring tletect\n",
      "selftud prals.\n",
      " \n",
      "----\n",
      "iter 34000, loss: 45.969163\n",
      "----\n",
      " aly souls. \"6hat commer bifih it seck wh see encastoveing s other\n",
      "yo strogred ther any user youbre. Houghaly, his lithing he other doce thentt, You himpare clope in nilk in waats himsulf he was slenes \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 43.787056\n",
      "----\n",
      " s mopent extem ono remerote, ere was buckme if this dy her gide way to anone toout find all nake even they al helms at, he  somest techow, imit was whened batslepine re to do speismive mit. Has ay he  \n",
      "----\n",
      "iter 36000, loss: 42.366634\n",
      "----\n",
      "  you he was sredid of herus .  but blid oacl in shout, ind, his sall in awe ustren the clice iming wating door soment had hin, kiup, his from hand you sot not impraw ly fright was layes to hand in tha \n",
      "----\n",
      "iter 37000, loss: 42.151987\n",
      "----\n",
      "  \"Iverted to to comhing sam's noveblo's, to the roon themstob, her -tled, as alatyor. The listor howlecking the tome. Whour, he, soiting higlays of at homnud of the itooning to and on from exshk movel \n",
      "----\n",
      "iter 38000, loss: 44.108989\n",
      "----\n",
      " ut youct Guterone go aigiC wore maUs inforiiges\" :U\n",
      "He poor plocgecherd ot\n",
      "regsting he ince Corpion only thisjear; Projects for and aghack she Pro work wis medion\n",
      "\"Whe sours a Projeglily were the Uwan \n",
      "----\n",
      "iter 39000, loss: 46.221548\n",
      "----\n",
      " dice but him bedaine siggs boch an his had modily for aw intion a out qon't even. Aver the from oB in sertie the icllout\n",
      " Arlose to as unfortivenge femaled nove of digh nouin wACly berad beramend fron \n",
      "----\n",
      "iter 40000, loss: 44.164415\n",
      "----\n",
      " y a digiws in over on awing pleads an re thoupmed but'r I lat infurrersing, to he had frevedsacf Gregor he sen to wate himself trojees, his sen that had hey with of the with a mingat bew that raan not \n",
      "----\n",
      "iter 41000, loss: 42.395357\n",
      "----\n",
      "  in nothing that been dare? Nok, juse morf be him get the anding he rould out as oning llvough slofsteped as lioned wores, he houre bear him dorryersin, the say it had his haver his boring alms at in  \n",
      "----\n",
      "iter 42000, loss: 42.016035\n",
      "----\n",
      " n his itionhing his a he  3ear duresple, have god - agheit - out en but of ento the quick to see the meat wou do or to awly up the was paturatsowss in, as en out to check, every yout laked ther the wo \n",
      "----\n",
      "iter 43000, loss: 41.812065\n",
      "----\n",
      " l woucrastind and his sisteeding his swen dork it chon, Greg domet has shis his wesh mows and of tret the furply, ditter they a\"wast Gregor but look breil the fane near he puct. Samaly the cone to oil \n",
      "----\n",
      "iter 44000, loss: 46.201092\n",
      "----\n",
      " ound wrist, pritly costeng(s\n",
      "s indargendoniverd, cas copy at Decenz\n",
      "and, and exiw slidyting not inuad wotherion mupwers and unover cumpole\n",
      "sne starf was rexprest thas oned clesplore une,\n",
      "  mownearesat \n",
      "----\n",
      "iter 45000, loss: 44.831310\n",
      "----\n",
      " n; her od the doursulx alldmenbe seet had dos out lising so frond sece and ailer ther hey this sood but difter she werk that sunting the need se't all more is a could tromeso and his moth mintt.\n",
      "\n",
      "Ther \n",
      "----\n",
      "iter 46000, loss: 42.773966\n",
      "----\n",
      " thes been, diding door be compead alved a as sinttcoile to as of geibull what wand to the peselresh, beey whan stosto every halb even vorin, wolly cainitate best as siting. But praked hatsiling onter  \n",
      "----\n",
      "iter 47000, loss: 41.405356\n",
      "----\n",
      " ay abound was soun ould fpen a Gregor?\"Asler othat, wuther, had stargs, for ntathing fur to to wheed, bott Gutilg that have untthing, food sforate tike wantplen was asmed whards the didly stowd and re \n",
      "----\n",
      "iter 48000, loss: 41.242344\n",
      "----\n",
      " he stable s taks radmait, ald thoure at ined, fur what him in corqurned in all ofted yought best caon for from he) ying loveed of the mond thought reaven nowald her her to this had look, while alerid  \n",
      "----\n",
      "iter 49000, loss: 43.021738\n",
      "----\n",
      "  tiGde s heres thealn in - to were aspyincingo the why to\n",
      "'s hop ortelisiy weUsese\n",
      "eny de who ing\n",
      "gothers of a bos sid every dointerony\n",
      "\n",
      "\n",
      "O*N/Proulns. Grent ito withing cleapguly hat Project Guted to  \n",
      "----\n",
      "iter 50000, loss: 45.167113\n",
      "----\n",
      " lo'ntion, \" Yound conthy  uor the dongemitem read\n",
      "nemut wullly partsen whrat and supplintssaal effore, to requivins havelle\n",
      "ce could came faching but to then that mestat of he ofdo a sthe to .  th tli \n",
      "----\n",
      "iter 51000, loss: 43.287634\n",
      "----\n",
      " y as befhere was farsibrems. But now Df thht the pravely us him to having it wanLid they wa would strow bod; he fullintly, the stick his uncamas; ho destelidy the lopmelly he suntentirally a fulle so  \n",
      "----\n",
      "iter 52000, loss: 41.584307\n",
      "----\n",
      " e had toresseacescoutenten to could wand had she ken seres race but cure then stoor in his nothceac loiring e ht wrowang he pather, he would his sere the wungd of then, fest brome, ih hore he disise n \n",
      "----\n",
      "iter 53000, loss: 41.296432\n",
      "----\n",
      " ensoy. Gregorss him lovens leking rullebly heme tice, . Ath her waster and be him, any, anyonkne illoway for remurd non, ppentel the for sifterrsentment. Freentmists with she his hamself whit it in th \n",
      "----\n",
      "iter 54000, loss: 41.068326\n",
      "----\n",
      " o for this not in the room. He save kster siwto h. Him's she sto dech that his mother do one the toreif he hamse, proveid as he hould have one tert on the clud, at in marey hull e.E. Whow. No do rorea \n",
      "----\n",
      "iter 55000, loss: 45.026757\n",
      "----\n",
      " s.  Dovers conlupp be tard e atid con anct of fincreshen\n",
      "ear a quintong tunged unted ack the - must to sion and\n",
      "shis privedall to a copariras porvibles gat/le thin. IN ASh\n",
      "1/s ang - beto monseverepion \n",
      "----\n",
      "iter 56000, loss: 43.976697\n",
      "----\n",
      " tardes looking rust the. Ardway wastem traved had fertliggon, it his,, her exsing, out thrich jeachos, -t purfuling she wither and not chen shake to inqore, was should that conlee be usgintioute there \n",
      "----\n",
      "iter 57000, loss: 42.000202\n",
      "----\n",
      " , nicked, she pary could and woule to be of to shele, aw on flises? works throwaply; undeant theid of than soney overesury he had then the door no ansher woreation everytenbrest heach way all and waim \n",
      "----\n",
      "iter 58000, loss: 40.729705\n",
      "----\n",
      " f dowing go Gout and aschowly hand, shetem find his, \"Gd bed fell than mise st with the plafing, was nother mleirfuid that had he room with filled she gat if.-. \"And other and had for him be had beed  \n",
      "----\n",
      "iter 59000, loss: 40.579004\n",
      "----\n",
      " tely had mick in was movelyons s his suikforned him on, was bark the go wust would seotepy rofhirabrebletcrow; ; then, them.\n",
      "\n",
      "It his dall-te, even, well whisfun who whave onturms, he wantle was bythin \n",
      "----\n",
      "iter 60000, loss: 42.217210\n",
      "----\n",
      " he Gremoriss the corpen work nece and\"\n",
      "\n",
      "Mriggtiones Greathing al eftion falnso condy ha Praks ane any work, diss fann.\n",
      "\n",
      "UBway not or now sonying in apary lrbee on the dotar'  quicfoone ates, clesplid  \n",
      "----\n",
      "iter 61000, loss: 44.316439\n",
      "----\n",
      " berdlto \"  greaved ih, theccer be effortidovsistensall the Feme at the brimpled the Prod he'proter to the dangen/ar the nle in thet to chree that Gregor his fitheap, and him.\", he calme erpoles sed he \n",
      "----\n",
      "iter 62000, loss: 42.621448\n",
      "----\n",
      " n't ablut traksaback ot baclu los him, whon. \"Whound uning osto outeting they he carfoomes wangir pright, timenbers hrowd in mafted behard would statter ppaw wat aid  and becaugl if dack I dos noj had \n",
      "----\n",
      "iter 63000, loss: 40.966449\n",
      "----\n",
      " l at had was notion of now working, weir rameroning inandath ouss in conto, oe but but mest weat cound withwaten the come at heve any sooksieting thet mid the sat of agntaty, but her not as not pe tur \n",
      "----\n",
      "iter 64000, loss: 40.720140\n",
      "----\n",
      " , his. S lomar a locely,. He laraang youlds he ctuld atoor be suched's furter's potrent him the had that make and lrawly foon, had as at as she waat the whonited owe of an' lighoppayibidy, wore, - mol \n",
      "----\n",
      "iter 65000, loss: 40.484676\n",
      "----\n",
      " hought. Wise worriot. She san's jace deakes becour bufno any the sly cousd as aboules was chiver she floor at than bus at dosto, and soref and ag?\"S\", slaw of hean Gregreghing dean the doorgaingone lo \n",
      "----\n",
      "iter 66000, loss: 44.109299\n",
      "----\n",
      " tl the pormecm so wrongone gecked broll comings the Praking\n",
      "with resurro\n",
      "Pheare lionsaon\n",
      " $accly of forminds of toned orcamprate0ntute lests this lotked dirtinat to\n",
      "\n",
      "- ted liondon and as Project - mea \n",
      "----\n",
      "iter 67000, loss: 43.285912\n",
      "----\n",
      " sthe contld to enticare hoid in be seel nt op do the as ald stowsy.  homatked hird inceinted his hay edsen termy and now\"s\", hushedsing and he was oth liftrepars. Spation thone hey med anvertiounonaly \n",
      "----\n",
      "iter 68000, loss: 41.401232\n",
      "----\n",
      " ne tion as he taong, as at pare, an: sert, sometaid Gregor hem he could could his badto that minedid they recenlefod mimself they medsentibationst and it srying longed would as no- im ouls thatsed pro \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 40.190994\n",
      "----\n",
      " aft't fited lit be him more cive threains. Wo Pay the stat the dade didaly had thensh awout, Grehe. Jund stasted had been stust one tistun in at fur even and onton. Hiss pound oming; and of that wring \n",
      "----\n",
      "iter 70000, loss: 40.029754\n",
      "----\n",
      "  to entulr cously him at they cupsenced he sertoy on the whackent plooving as lo-teaking rooms the would Gretay to a she not in the chaod woor a livlo stimskithtry himsppeonb: Not he had were no- toor \n",
      "----\n",
      "iter 71000, loss: 41.606160\n",
      "----\n",
      " ested inday beed the e)for move the Lay at feered fip have rease to Preplete a wanthing. \n",
      "HI\n",
      " The com. Sawatch, by\n",
      "y be spetted ucray. \"Projert uncelo\n",
      "eren in the comer and but to Gregor's free the co \n",
      "----\n",
      "iter 72000, loss: 43.593088\n",
      "----\n",
      " ly Thead bading excus fleakly his fothed but him but who to stut whel ditietisunet, he caad come be stardice her his corplay and even theritecto conger to nicher texpemed he time a mides himall tor'pl \n",
      "----\n",
      "iter 73000, loss: 42.012353\n",
      "----\n",
      " nfing beosed had to suint if lidlo his father with to mowsine the opeid at thouves head anfel the chewn's his effor litter prenny how, leok, it Gregersel if st move caan, sown the semas, she us inhial \n",
      "----\n",
      "iter 74000, loss: 40.436704\n",
      "----\n",
      "  slat out shing to watserast rusafiom wus seeply bet-wur to, have teelee it\n",
      "work any. Thoway get ead reainging that meth thomiable murar and hearen every may ob it as were broouth tich ont assard, his \n",
      "----\n",
      "iter 75000, loss: 40.271869\n",
      "----\n",
      " st; he came hard of exirins genvion herwonch hel apretily.\n",
      " he lork be her out movery, he dus deonf eler the chare. weer when\n",
      " he Panger mome the wlike - or with can't issuih appen straighnumusion but \n",
      "----\n",
      "iter 76000, loss: 40.008753\n",
      "----\n",
      "  deast to his foed hicked assed, and iffipoons his, atchinkieging, outss, even wher was the thraging, terfor her wourding come the doles\n",
      "sredbeece abeed mastolkne, with repare allcllo veriuppead her m \n",
      "----\n",
      "iter 77000, loss: 43.206651\n",
      "----\n",
      " posice to e.  The sacceming and cabol (Junde ants and to the wouvlo cer Foundang-tm\n",
      ".\n",
      " - Projectm. Sampard (F)en eringong and bucemevers. Fas seade. Proje fintore the tire way sfock the fulnter\n",
      "plen\n",
      "t \n",
      "----\n",
      "iter 78000, loss: 42.704665\n",
      "----\n",
      " ing then nos whick. \"And, yourre was he pathen Projion ying, and\", key Gregor's room.  an of think awfare for doonto, caaken lay w. sert loor tordstion's anch ruchandnen? I'mone AwsacyiDgon'n.\n",
      "\n",
      "\"Wat s \n",
      "----\n",
      "iter 79000, loss: 40.885507\n",
      "----\n",
      " at listendent tects, thout no growler to floce, at ot toomen-ly of one of he torngo ensicereat that he cample text couminging, coping it, the mated to simit bad tmostoe evendigention anb, - then sidy. \n",
      "----\n",
      "iter 80000, loss: 39.735837\n",
      "----\n",
      " had his been him and cove!\" housed of eached you to mess. Has sllore rigd overy he had statistuns; he Chath his doss to and not. The vimispar that the other al, doound to be roatly his hamself he e mo \n",
      "----\n",
      "iter 81000, loss: 39.599577\n",
      "----\n",
      " that paid pleaght but with he ladando imusest Gregor case with they weer hay to mey about for to whight e-this fithing would soor, as sayseress; mo room the pearing them be to moved he capention in wi \n",
      "----\n",
      "iter 82000, loss: 41.042747\n",
      "----\n",
      " ede and cheok eBes.\n",
      "\n",
      "\"Peccho, with recaiches litiam:\n",
      "\n",
      " Project Gucgemoncerchompecce geve of sirsad all of thrutelutirard; thet toone congery ang, and the tooked agrilation sroverayitionsurs\n",
      "wertil thi \n",
      "----\n",
      "iter 83000, loss: 42.997671\n",
      "----\n",
      " uster the thay shated no you not he had beding it ing his thit he gow tolkeall tooutIthing for alded to kidage pistows linded thnichy bed to hip himsils loaking that tile, hincch of they he eneteed he \n",
      "----\n",
      "iter 84000, loss: 41.571811\n",
      "----\n",
      " sed lothing the door's hwarr ryoudiser, light sting said graytlicemanding saand had had lround witr can aswough, And to fake sinded night sorsiy but aLy was his fay a dutlint Gregor's bayVuse faine, b \n",
      "----\n",
      "iter 85000, loss: 39.994065\n",
      "----\n",
      " ler abouffeen havely to see what ludding at movenly, was Gregor's alreite gelked she liching extonbions. It only it everyoupm, se fire he sure and proght sroved of merienvion his seally to caisectayin \n",
      "----\n",
      "iter 86000, loss: 39.925213\n",
      "----\n",
      " f, in wand, a gome, he harkt by set fell to as of his forse troped if he sad, it thad lister did compout of exhardotes ped he baiss thomb had pean who byen had fatceer cample notreach pees. The commar \n",
      "----\n",
      "iter 87000, loss: 39.616342\n",
      "----\n",
      " raded I sthe core spathither, he onough, e sore there thoudd, an ded oth frow his makiry foridly his foresy he sake sourt'tly cove murte of her eary now hear she cayilln ele ither one radrespall chary \n",
      "----\n",
      "iter 88000, loss: 42.508094\n",
      "----\n",
      " her, unbee are to the tintulistery.  Proght The obither ( accusing beer that loth taxicact of to in and ustectired impsiffice ceavlidate and ablid but think but heled be thoughas go and to\n",
      "thing creel \n",
      "----\n",
      "iter 89000, loss: 42.216811\n",
      "----\n",
      " arsing of for aswough de.\". I'm. Be as docksipien the know\", somsly, be quiety, she staffer on his meplargaitleinging effort case and dester the could. Whandent in theres have thrigent, slikeser only  \n",
      "----\n",
      "iter 90000, loss: 40.429336\n",
      "----\n",
      " tly packides caden open dast that a aid sound haves inougho again de gocssing alled in the tood did so sening, the sametboub pack himss a bod thind Gregorked but forteres his waites thempenschind of s \n",
      "----\n",
      "iter 91000, loss: 39.443791\n",
      "----\n",
      " xther, alectiched to the had the sungir, spich was fouct onhidarost int and some apping the movely thave repares he's their aach the stow a for moved simaly!\" The andy father overatront go stot't he h \n",
      "----\n",
      "iter 92000, loss: 39.243439\n",
      "----\n",
      " eed, but thrien lusiabte te the tore of the coshevened teve was stainely when was slace sime him weat meatemsavily mone, so moving mis), whet inthriosel\", sound with strought not in - not lorsermefman \n",
      "----\n",
      "iter 93000, loss: 40.565547\n",
      "----\n",
      " east intarm to fathing and being the cos wrelasfor as funing.\n",
      " S**\n",
      "TNESE ASt GUTp arkalvention muct madriach wead theyemesplit-in in the congon)\n",
      "bietly, room.\n",
      "\n",
      "Tlose turs with quiin to weren layiefvim \n",
      "----\n",
      "iter 94000, loss: 42.492293\n",
      "----\n",
      " just that up the I light house bectotiont besting here of qusing expesible not one if thit wolls a contlered withalces shate his fith is forsment with this alls, could seceredong not up he ofted to di \n",
      "----\n",
      "iter 95000, loss: 41.172400\n",
      "----\n",
      " un hen slance naw restread get to puivelren her ksay did wore, firpreeds it thiakeran uppeelf from Gregory. The faring, jutter's chiaks but the forllly matsed murned thrwait of the otherp migely prae  \n",
      "----\n",
      "iter 96000, loss: 39.663858\n",
      "----\n",
      "  refor!a gath, he could net atsely as ommas used have on the sire evenyted they. Gregor the said state ansull\n",
      "allow some beit; with lothing to Gretab. He \"he was not wuthough my alr. He pith pristed s \n",
      "----\n",
      "iter 97000, loss: 39.637153\n",
      "----\n",
      " resion allo, then that, eary for chabisiad at ground gaver then?Jums. It of hels. \"Gopgery and jumped shen if yherey, and khyather, be at whare the sothering to neft open which eusing wrece pusie, him \n",
      "----\n",
      "iter 98000, loss: 39.317151\n",
      "----\n",
      "  oncroped work at.\n",
      "\n",
      "It Greance. Her sasibed her lork had was heirer thkirly digsed straw mundly grougrnost to ensesebanaged that his mother had the varen thele as loining his mather or agread.\n",
      "\n",
      "And al \n",
      "----\n",
      "iter 99000, loss: 41.935514\n",
      "----\n",
      " ellacle AtI dock the ranciots compat with as ceedly about lissec mon coppookstead and ismepered fromaring; any reation. 1.F AnTil neich and the compare Affliced. \n",
      "IRA55 POF lrojectw rey and tonating,  \n",
      "----\n",
      "iter 100000, loss: 41.753110\n",
      "----\n",
      " nd. They tawple up abous vad beh aratire of his thisk elen while seareray. Verassy slock his have \"or the quinut a wa stays in with the dighcl - would moving of rablled it of\n",
      ", e.\"Th. Dlo it and the g \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0 \n",
    "\n",
    "while n<=1000*100: # 1000*100:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                           \n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    p += seq_length # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
