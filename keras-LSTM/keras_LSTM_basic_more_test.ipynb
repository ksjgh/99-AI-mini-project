{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_LSTM_basic_more_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJXZFc4CN47m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# see https://youtu.be/iMIWee_PXl8\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVrGv0Gnxh02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data = [[ [(j+i)] for i in range(5)] for j in range(100) ]\n",
        "data = [[ [(j+i)/100] for i in range(5)] for j in range(100) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyMax_TyyCJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f612cde5-123f-4cc3-88cc-b97837397fad"
      },
      "source": [
        "data[:10]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.0], [0.01], [0.02], [0.03], [0.04]],\n",
              " [[0.01], [0.02], [0.03], [0.04], [0.05]],\n",
              " [[0.02], [0.03], [0.04], [0.05], [0.06]],\n",
              " [[0.03], [0.04], [0.05], [0.06], [0.07]],\n",
              " [[0.04], [0.05], [0.06], [0.07], [0.08]],\n",
              " [[0.05], [0.06], [0.07], [0.08], [0.09]],\n",
              " [[0.06], [0.07], [0.08], [0.09], [0.1]],\n",
              " [[0.07], [0.08], [0.09], [0.1], [0.11]],\n",
              " [[0.08], [0.09], [0.1], [0.11], [0.12]],\n",
              " [[0.09], [0.1], [0.11], [0.12], [0.13]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWrn5BCGyE4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = [ (i+5)/100 for i in range(100) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68IX7Ix1yHsY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db1bd2d7-7df0-4d89-f418-ff819789b57d"
      },
      "source": [
        "target[:10]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50z2ITr1GVCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test for variable sequence length\n",
        "# data = [[ [(j+i)/100] for i in range(5)] for j in range(100) ]\n",
        "# target = [ (i++5)/100 for i in range(100) ]\n",
        "\n",
        "data = [[ [(j+i)/100] for i in range(7)] for j in range(100) ]\n",
        "target = [ (i+7)/100 for i in range(100) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxuNb4yBydEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.array(data,dtype=float)\n",
        "target = np.array(target,dtype=float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn2Diy60y-ap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce4d9ea1-7428-4a7e-f7dc-65992c0dd82b"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 7, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-CUxW1AzBGw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3207436-1bd9-4b0a-da99-8fd08a75adfd"
      },
      "source": [
        "target.shape"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNjT1m9PzC0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train , x_test , y_train, y_test = train_test_split( data, target, test_size = 0.2, random_state=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yllMZNKczzMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyCJy7t11pJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.add( LSTM((1), batch_input_shape = (None,5,1), return_sequences=True) )\n",
        "model.add( LSTM((4), batch_input_shape = (None,None,1), return_sequences=True) )\n",
        "model.add( LSTM((4), batch_input_shape = (None,None,1), return_sequences=True) )\n",
        "model.add( LSTM((1), return_sequences=False ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p397ZE3O1_nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"mean_absolute_error\", optimizer = \"adam\", metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWkI1s-Z2Y8H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b05bcf77-129e-448b-a023-9e6facb65465"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_20 (LSTM)               (None, None, 4)           96        \n",
            "_________________________________________________________________\n",
            "lstm_21 (LSTM)               (None, None, 4)           144       \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 1)                 24        \n",
            "=================================================================\n",
            "Total params: 264\n",
            "Trainable params: 264\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z78NVaPuaL1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "a20e97e9-d565-4ca4-9c9e-a5b46b13594f"
      },
      "source": [
        "#  Display Model in Jupyter notebook\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "def plot_keras_model(model,show_shapes=True,show_layer_names=True):\n",
        "    return SVG(model_to_dot(model,show_shapes=show_shapes,show_layer_names=show_layer_names).create(prog='dot',format='svg'))\n",
        "\n",
        "plot_keras_model(model, show_shapes=True, show_layer_names=False)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"294pt\" viewBox=\"0.00 0.00 233.00 294.00\" width=\"233pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 290)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-290 229,-290 229,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140368111348368 -->\n<g class=\"node\" id=\"node1\">\n<title>140368111348368</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 225,-212.5 225,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"28\" y=\"-185.8\">LSTM</text>\n<polyline fill=\"none\" points=\"56,-166.5 56,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"56,-189.5 114,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"114,-166.5 114,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-197.3\">(None, None, 1)</text>\n<polyline fill=\"none\" points=\"114,-189.5 225,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-174.3\">(None, None, 4)</text>\n</g>\n<!-- 140368111348872 -->\n<g class=\"node\" id=\"node2\">\n<title>140368111348872</title>\n<polygon fill=\"none\" points=\"0,-83.5 0,-129.5 225,-129.5 225,-83.5 0,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"28\" y=\"-102.8\">LSTM</text>\n<polyline fill=\"none\" points=\"56,-83.5 56,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"56,-106.5 114,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"114,-83.5 114,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-114.3\">(None, None, 4)</text>\n<polyline fill=\"none\" points=\"114,-106.5 225,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-91.3\">(None, None, 4)</text>\n</g>\n<!-- 140368111348368&#45;&gt;140368111348872 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140368111348368-&gt;140368111348872</title>\n<path d=\"M112.5,-166.3799C112.5,-158.1745 112.5,-148.7679 112.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"116.0001,-139.784 112.5,-129.784 109.0001,-139.784 116.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140368111310160 -->\n<g class=\"node\" id=\"node3\">\n<title>140368111310160</title>\n<polygon fill=\"none\" points=\"0,-.5 0,-46.5 225,-46.5 225,-.5 0,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"28\" y=\"-19.8\">LSTM</text>\n<polyline fill=\"none\" points=\"56,-.5 56,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"56,-23.5 114,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"114,-.5 114,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-31.3\">(None, None, 4)</text>\n<polyline fill=\"none\" points=\"114,-23.5 225,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"169.5\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 140368111348872&#45;&gt;140368111310160 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140368111348872-&gt;140368111310160</title>\n<path d=\"M112.5,-83.3799C112.5,-75.1745 112.5,-65.7679 112.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"116.0001,-56.784 112.5,-46.784 109.0001,-56.784 116.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140368111388880 -->\n<g class=\"node\" id=\"node4\">\n<title>140368111388880</title>\n<polygon fill=\"none\" points=\"48.5,-249.5 48.5,-285.5 176.5,-285.5 176.5,-249.5 48.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.5\" y=\"-263.8\">140368111388880</text>\n</g>\n<!-- 140368111388880&#45;&gt;140368111348368 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140368111388880-&gt;140368111348368</title>\n<path d=\"M112.5,-249.4092C112.5,-241.4308 112.5,-231.795 112.5,-222.606\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"116.0001,-222.5333 112.5,-212.5333 109.0001,-222.5334 116.0001,-222.5333\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-WdxVm92bWO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10234
        },
        "outputId": "e0f822f7-72eb-4f5a-9e28-c070a1dcc09b"
      },
      "source": [
        "history = model.fit( x_train, y_train, epochs=300, validation_data=(x_test,y_test) )"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 80 samples, validate on 20 samples\n",
            "Epoch 1/300\n",
            "80/80 [==============================] - 5s 61ms/step - loss: 0.5856 - acc: 0.0000e+00 - val_loss: 0.4667 - val_acc: 0.0000e+00\n",
            "Epoch 2/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5718 - acc: 0.0000e+00 - val_loss: 0.4541 - val_acc: 0.0000e+00\n",
            "Epoch 3/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5585 - acc: 0.0000e+00 - val_loss: 0.4416 - val_acc: 0.0000e+00\n",
            "Epoch 4/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5449 - acc: 0.0000e+00 - val_loss: 0.4289 - val_acc: 0.0000e+00\n",
            "Epoch 5/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5316 - acc: 0.0000e+00 - val_loss: 0.4161 - val_acc: 0.0000e+00\n",
            "Epoch 6/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5179 - acc: 0.0000e+00 - val_loss: 0.4031 - val_acc: 0.0000e+00\n",
            "Epoch 7/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.5040 - acc: 0.0000e+00 - val_loss: 0.3898 - val_acc: 0.0000e+00\n",
            "Epoch 8/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4898 - acc: 0.0000e+00 - val_loss: 0.3762 - val_acc: 0.0000e+00\n",
            "Epoch 9/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4759 - acc: 0.0000e+00 - val_loss: 0.3624 - val_acc: 0.0000e+00\n",
            "Epoch 10/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4616 - acc: 0.0000e+00 - val_loss: 0.3494 - val_acc: 0.0000e+00\n",
            "Epoch 11/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4474 - acc: 0.0000e+00 - val_loss: 0.3362 - val_acc: 0.0000e+00\n",
            "Epoch 12/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4336 - acc: 0.0000e+00 - val_loss: 0.3227 - val_acc: 0.0000e+00\n",
            "Epoch 13/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.4193 - acc: 0.0000e+00 - val_loss: 0.3091 - val_acc: 0.0000e+00\n",
            "Epoch 14/300\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4065 - acc: 0.0000e+00 - val_loss: 0.2954 - val_acc: 0.0000e+00\n",
            "Epoch 15/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3934 - acc: 0.0000e+00 - val_loss: 0.2820 - val_acc: 0.0000e+00\n",
            "Epoch 16/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3805 - acc: 0.0000e+00 - val_loss: 0.2697 - val_acc: 0.0000e+00\n",
            "Epoch 17/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3690 - acc: 0.0000e+00 - val_loss: 0.2578 - val_acc: 0.0000e+00\n",
            "Epoch 18/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3572 - acc: 0.0000e+00 - val_loss: 0.2480 - val_acc: 0.0000e+00\n",
            "Epoch 19/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3460 - acc: 0.0000e+00 - val_loss: 0.2393 - val_acc: 0.0000e+00\n",
            "Epoch 20/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3354 - acc: 0.0000e+00 - val_loss: 0.2317 - val_acc: 0.0000e+00\n",
            "Epoch 21/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3253 - acc: 0.0000e+00 - val_loss: 0.2244 - val_acc: 0.0000e+00\n",
            "Epoch 22/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.3155 - acc: 0.0000e+00 - val_loss: 0.2181 - val_acc: 0.0000e+00\n",
            "Epoch 23/300\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.3073 - acc: 0.0000e+00 - val_loss: 0.2125 - val_acc: 0.0000e+00\n",
            "Epoch 24/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2993 - acc: 0.0000e+00 - val_loss: 0.2072 - val_acc: 0.0000e+00\n",
            "Epoch 25/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2916 - acc: 0.0000e+00 - val_loss: 0.2032 - val_acc: 0.0000e+00\n",
            "Epoch 26/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2856 - acc: 0.0000e+00 - val_loss: 0.2001 - val_acc: 0.0000e+00\n",
            "Epoch 27/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2794 - acc: 0.0000e+00 - val_loss: 0.1977 - val_acc: 0.0000e+00\n",
            "Epoch 28/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2738 - acc: 0.0000e+00 - val_loss: 0.1956 - val_acc: 0.0000e+00\n",
            "Epoch 29/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2691 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0000e+00\n",
            "Epoch 30/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2645 - acc: 0.0000e+00 - val_loss: 0.1935 - val_acc: 0.0000e+00\n",
            "Epoch 31/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2603 - acc: 0.0000e+00 - val_loss: 0.1926 - val_acc: 0.0000e+00\n",
            "Epoch 32/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2569 - acc: 0.0000e+00 - val_loss: 0.1916 - val_acc: 0.0000e+00\n",
            "Epoch 33/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2537 - acc: 0.0000e+00 - val_loss: 0.1907 - val_acc: 0.0000e+00\n",
            "Epoch 34/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2504 - acc: 0.0000e+00 - val_loss: 0.1898 - val_acc: 0.0000e+00\n",
            "Epoch 35/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2480 - acc: 0.0000e+00 - val_loss: 0.1896 - val_acc: 0.0000e+00\n",
            "Epoch 36/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2453 - acc: 0.0000e+00 - val_loss: 0.1893 - val_acc: 0.0000e+00\n",
            "Epoch 37/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2430 - acc: 0.0000e+00 - val_loss: 0.1894 - val_acc: 0.0000e+00\n",
            "Epoch 38/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2407 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0000e+00\n",
            "Epoch 39/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2387 - acc: 0.0125 - val_loss: 0.1899 - val_acc: 0.0000e+00\n",
            "Epoch 40/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2365 - acc: 0.0125 - val_loss: 0.1900 - val_acc: 0.0000e+00\n",
            "Epoch 41/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2347 - acc: 0.0125 - val_loss: 0.1900 - val_acc: 0.0000e+00\n",
            "Epoch 42/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2329 - acc: 0.0125 - val_loss: 0.1899 - val_acc: 0.0000e+00\n",
            "Epoch 43/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2312 - acc: 0.0125 - val_loss: 0.1898 - val_acc: 0.0000e+00\n",
            "Epoch 44/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2295 - acc: 0.0125 - val_loss: 0.1899 - val_acc: 0.0000e+00\n",
            "Epoch 45/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2277 - acc: 0.0125 - val_loss: 0.1900 - val_acc: 0.0000e+00\n",
            "Epoch 46/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2261 - acc: 0.0125 - val_loss: 0.1900 - val_acc: 0.0000e+00\n",
            "Epoch 47/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2248 - acc: 0.0125 - val_loss: 0.1901 - val_acc: 0.0000e+00\n",
            "Epoch 48/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2231 - acc: 0.0125 - val_loss: 0.1901 - val_acc: 0.0000e+00\n",
            "Epoch 49/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2213 - acc: 0.0125 - val_loss: 0.1901 - val_acc: 0.0000e+00\n",
            "Epoch 50/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2198 - acc: 0.0125 - val_loss: 0.1902 - val_acc: 0.0000e+00\n",
            "Epoch 51/300\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.2183 - acc: 0.0125 - val_loss: 0.1902 - val_acc: 0.0000e+00\n",
            "Epoch 52/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2167 - acc: 0.0125 - val_loss: 0.1901 - val_acc: 0.0000e+00\n",
            "Epoch 53/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2151 - acc: 0.0125 - val_loss: 0.1898 - val_acc: 0.0000e+00\n",
            "Epoch 54/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2135 - acc: 0.0125 - val_loss: 0.1893 - val_acc: 0.0000e+00\n",
            "Epoch 55/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2118 - acc: 0.0125 - val_loss: 0.1885 - val_acc: 0.0000e+00\n",
            "Epoch 56/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2103 - acc: 0.0125 - val_loss: 0.1878 - val_acc: 0.0000e+00\n",
            "Epoch 57/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2085 - acc: 0.0125 - val_loss: 0.1867 - val_acc: 0.0000e+00\n",
            "Epoch 58/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2068 - acc: 0.0125 - val_loss: 0.1858 - val_acc: 0.0000e+00\n",
            "Epoch 59/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2051 - acc: 0.0125 - val_loss: 0.1848 - val_acc: 0.0000e+00\n",
            "Epoch 60/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2032 - acc: 0.0125 - val_loss: 0.1835 - val_acc: 0.0000e+00\n",
            "Epoch 61/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.2013 - acc: 0.0125 - val_loss: 0.1820 - val_acc: 0.0000e+00\n",
            "Epoch 62/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1993 - acc: 0.0125 - val_loss: 0.1801 - val_acc: 0.0000e+00\n",
            "Epoch 63/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1973 - acc: 0.0125 - val_loss: 0.1778 - val_acc: 0.0000e+00\n",
            "Epoch 64/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1951 - acc: 0.0125 - val_loss: 0.1755 - val_acc: 0.0000e+00\n",
            "Epoch 65/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1927 - acc: 0.0125 - val_loss: 0.1732 - val_acc: 0.0000e+00\n",
            "Epoch 66/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1904 - acc: 0.0125 - val_loss: 0.1707 - val_acc: 0.0000e+00\n",
            "Epoch 67/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1879 - acc: 0.0125 - val_loss: 0.1678 - val_acc: 0.0000e+00\n",
            "Epoch 68/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1851 - acc: 0.0125 - val_loss: 0.1648 - val_acc: 0.0000e+00\n",
            "Epoch 69/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1824 - acc: 0.0125 - val_loss: 0.1613 - val_acc: 0.0000e+00\n",
            "Epoch 70/300\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.1794 - acc: 0.0125 - val_loss: 0.1576 - val_acc: 0.0000e+00\n",
            "Epoch 71/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1763 - acc: 0.0125 - val_loss: 0.1537 - val_acc: 0.0000e+00\n",
            "Epoch 72/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1729 - acc: 0.0125 - val_loss: 0.1496 - val_acc: 0.0000e+00\n",
            "Epoch 73/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1694 - acc: 0.0125 - val_loss: 0.1454 - val_acc: 0.0000e+00\n",
            "Epoch 74/300\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.1654 - acc: 0.0125 - val_loss: 0.1410 - val_acc: 0.0000e+00\n",
            "Epoch 75/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1616 - acc: 0.0125 - val_loss: 0.1363 - val_acc: 0.0000e+00\n",
            "Epoch 76/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1575 - acc: 0.0125 - val_loss: 0.1309 - val_acc: 0.0000e+00\n",
            "Epoch 77/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1530 - acc: 0.0125 - val_loss: 0.1254 - val_acc: 0.0000e+00\n",
            "Epoch 78/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1482 - acc: 0.0125 - val_loss: 0.1198 - val_acc: 0.0000e+00\n",
            "Epoch 79/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1432 - acc: 0.0125 - val_loss: 0.1140 - val_acc: 0.0000e+00\n",
            "Epoch 80/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1379 - acc: 0.0125 - val_loss: 0.1080 - val_acc: 0.0000e+00\n",
            "Epoch 81/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1325 - acc: 0.0125 - val_loss: 0.1020 - val_acc: 0.0000e+00\n",
            "Epoch 82/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1266 - acc: 0.0125 - val_loss: 0.0956 - val_acc: 0.0000e+00\n",
            "Epoch 83/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1202 - acc: 0.0125 - val_loss: 0.0891 - val_acc: 0.0000e+00\n",
            "Epoch 84/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1134 - acc: 0.0125 - val_loss: 0.0825 - val_acc: 0.0000e+00\n",
            "Epoch 85/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.1064 - acc: 0.0125 - val_loss: 0.0759 - val_acc: 0.0000e+00\n",
            "Epoch 86/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0988 - acc: 0.0125 - val_loss: 0.0688 - val_acc: 0.0000e+00\n",
            "Epoch 87/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0908 - acc: 0.0125 - val_loss: 0.0603 - val_acc: 0.0000e+00\n",
            "Epoch 88/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0823 - acc: 0.0125 - val_loss: 0.0515 - val_acc: 0.0000e+00\n",
            "Epoch 89/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0745 - acc: 0.0125 - val_loss: 0.0500 - val_acc: 0.0000e+00\n",
            "Epoch 90/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0702 - acc: 0.0125 - val_loss: 0.0494 - val_acc: 0.0000e+00\n",
            "Epoch 91/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0657 - acc: 0.0125 - val_loss: 0.0518 - val_acc: 0.0000e+00\n",
            "Epoch 92/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0618 - acc: 0.0125 - val_loss: 0.0545 - val_acc: 0.0000e+00\n",
            "Epoch 93/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0601 - acc: 0.0125 - val_loss: 0.0557 - val_acc: 0.0000e+00\n",
            "Epoch 94/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0590 - acc: 0.0125 - val_loss: 0.0559 - val_acc: 0.0000e+00\n",
            "Epoch 95/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0573 - acc: 0.0125 - val_loss: 0.0553 - val_acc: 0.0000e+00\n",
            "Epoch 96/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0559 - acc: 0.0125 - val_loss: 0.0541 - val_acc: 0.0000e+00\n",
            "Epoch 97/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0544 - acc: 0.0125 - val_loss: 0.0527 - val_acc: 0.0000e+00\n",
            "Epoch 98/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0529 - acc: 0.0125 - val_loss: 0.0514 - val_acc: 0.0000e+00\n",
            "Epoch 99/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0516 - acc: 0.0125 - val_loss: 0.0504 - val_acc: 0.0000e+00\n",
            "Epoch 100/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0501 - acc: 0.0125 - val_loss: 0.0495 - val_acc: 0.0000e+00\n",
            "Epoch 101/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0486 - acc: 0.0125 - val_loss: 0.0486 - val_acc: 0.0000e+00\n",
            "Epoch 102/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0472 - acc: 0.0125 - val_loss: 0.0479 - val_acc: 0.0000e+00\n",
            "Epoch 103/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0458 - acc: 0.0125 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
            "Epoch 104/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0444 - acc: 0.0125 - val_loss: 0.0443 - val_acc: 0.0000e+00\n",
            "Epoch 105/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0429 - acc: 0.0125 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
            "Epoch 106/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0417 - acc: 0.0125 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
            "Epoch 107/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0405 - acc: 0.0125 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
            "Epoch 108/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0396 - acc: 0.0125 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
            "Epoch 109/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0382 - acc: 0.0125 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
            "Epoch 110/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0372 - acc: 0.0125 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
            "Epoch 111/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0362 - acc: 0.0125 - val_loss: 0.0376 - val_acc: 0.0000e+00\n",
            "Epoch 112/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0352 - acc: 0.0125 - val_loss: 0.0370 - val_acc: 0.0000e+00\n",
            "Epoch 113/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0341 - acc: 0.0125 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
            "Epoch 114/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0330 - acc: 0.0125 - val_loss: 0.0359 - val_acc: 0.0000e+00\n",
            "Epoch 115/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0322 - acc: 0.0125 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
            "Epoch 116/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0313 - acc: 0.0125 - val_loss: 0.0348 - val_acc: 0.0000e+00\n",
            "Epoch 117/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0305 - acc: 0.0125 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
            "Epoch 118/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0298 - acc: 0.0125 - val_loss: 0.0340 - val_acc: 0.0000e+00\n",
            "Epoch 119/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0292 - acc: 0.0125 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
            "Epoch 120/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0283 - acc: 0.0125 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
            "Epoch 121/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0278 - acc: 0.0125 - val_loss: 0.0318 - val_acc: 0.0000e+00\n",
            "Epoch 122/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0272 - acc: 0.0125 - val_loss: 0.0311 - val_acc: 0.0000e+00\n",
            "Epoch 123/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0266 - acc: 0.0125 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
            "Epoch 124/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0261 - acc: 0.0125 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
            "Epoch 125/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0256 - acc: 0.0125 - val_loss: 0.0293 - val_acc: 0.0000e+00\n",
            "Epoch 126/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0252 - acc: 0.0125 - val_loss: 0.0289 - val_acc: 0.0000e+00\n",
            "Epoch 127/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0246 - acc: 0.0125 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
            "Epoch 128/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0243 - acc: 0.0125 - val_loss: 0.0282 - val_acc: 0.0000e+00\n",
            "Epoch 129/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0238 - acc: 0.0125 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
            "Epoch 130/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0234 - acc: 0.0125 - val_loss: 0.0276 - val_acc: 0.0000e+00\n",
            "Epoch 131/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0230 - acc: 0.0125 - val_loss: 0.0277 - val_acc: 0.0000e+00\n",
            "Epoch 132/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0226 - acc: 0.0125 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
            "Epoch 133/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0225 - acc: 0.0125 - val_loss: 0.0273 - val_acc: 0.0000e+00\n",
            "Epoch 134/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0220 - acc: 0.0125 - val_loss: 0.0265 - val_acc: 0.0000e+00\n",
            "Epoch 135/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0216 - acc: 0.0125 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
            "Epoch 136/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0214 - acc: 0.0125 - val_loss: 0.0252 - val_acc: 0.0000e+00\n",
            "Epoch 137/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0209 - acc: 0.0125 - val_loss: 0.0249 - val_acc: 0.0000e+00\n",
            "Epoch 138/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0206 - acc: 0.0125 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
            "Epoch 139/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0205 - acc: 0.0125 - val_loss: 0.0249 - val_acc: 0.0000e+00\n",
            "Epoch 140/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0201 - acc: 0.0125 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
            "Epoch 141/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0199 - acc: 0.0125 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
            "Epoch 142/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0196 - acc: 0.0125 - val_loss: 0.0235 - val_acc: 0.0000e+00\n",
            "Epoch 143/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0193 - acc: 0.0125 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 144/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0194 - acc: 0.0125 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
            "Epoch 145/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0189 - acc: 0.0125 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
            "Epoch 146/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0186 - acc: 0.0125 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
            "Epoch 147/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0184 - acc: 0.0125 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
            "Epoch 148/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0183 - acc: 0.0125 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
            "Epoch 149/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0182 - acc: 0.0125 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
            "Epoch 150/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0179 - acc: 0.0125 - val_loss: 0.0212 - val_acc: 0.0000e+00\n",
            "Epoch 151/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0176 - acc: 0.0125 - val_loss: 0.0214 - val_acc: 0.0000e+00\n",
            "Epoch 152/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0174 - acc: 0.0125 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
            "Epoch 153/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0172 - acc: 0.0125 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
            "Epoch 154/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0181 - acc: 0.0125 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
            "Epoch 155/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0171 - acc: 0.0125 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
            "Epoch 156/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0168 - acc: 0.0125 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
            "Epoch 157/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0169 - acc: 0.0125 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
            "Epoch 158/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0163 - acc: 0.0125 - val_loss: 0.0211 - val_acc: 0.0000e+00\n",
            "Epoch 159/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0164 - acc: 0.0125 - val_loss: 0.0209 - val_acc: 0.0000e+00\n",
            "Epoch 160/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0161 - acc: 0.0125 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
            "Epoch 161/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0162 - acc: 0.0125 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
            "Epoch 162/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0157 - acc: 0.0125 - val_loss: 0.0197 - val_acc: 0.0000e+00\n",
            "Epoch 163/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0159 - acc: 0.0125 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
            "Epoch 164/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0159 - acc: 0.0125 - val_loss: 0.0183 - val_acc: 0.0000e+00\n",
            "Epoch 165/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0155 - acc: 0.0125 - val_loss: 0.0183 - val_acc: 0.0000e+00\n",
            "Epoch 166/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0154 - acc: 0.0125 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
            "Epoch 167/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0156 - acc: 0.0125 - val_loss: 0.0186 - val_acc: 0.0000e+00\n",
            "Epoch 168/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0150 - acc: 0.0125 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
            "Epoch 169/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0155 - acc: 0.0125 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 170/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0151 - acc: 0.0125 - val_loss: 0.0184 - val_acc: 0.0000e+00\n",
            "Epoch 171/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0154 - acc: 0.0125 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
            "Epoch 172/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0147 - acc: 0.0125 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 173/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0152 - acc: 0.0125 - val_loss: 0.0170 - val_acc: 0.0000e+00\n",
            "Epoch 174/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0148 - acc: 0.0125 - val_loss: 0.0179 - val_acc: 0.0000e+00\n",
            "Epoch 175/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0147 - acc: 0.0125 - val_loss: 0.0174 - val_acc: 0.0000e+00\n",
            "Epoch 176/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0147 - acc: 0.0125 - val_loss: 0.0170 - val_acc: 0.0000e+00\n",
            "Epoch 177/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0143 - acc: 0.0125 - val_loss: 0.0177 - val_acc: 0.0000e+00\n",
            "Epoch 178/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0144 - acc: 0.0125 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
            "Epoch 179/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0143 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 180/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0146 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 181/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0149 - acc: 0.0125 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
            "Epoch 182/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0146 - acc: 0.0125 - val_loss: 0.0170 - val_acc: 0.0000e+00\n",
            "Epoch 183/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0144 - acc: 0.0125 - val_loss: 0.0179 - val_acc: 0.0000e+00\n",
            "Epoch 184/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0142 - acc: 0.0125 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 185/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0140 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 186/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0138 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 187/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0137 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 188/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0136 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 189/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0137 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 190/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0135 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 191/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0139 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 192/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0136 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 193/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0136 - acc: 0.0125 - val_loss: 0.0174 - val_acc: 0.0000e+00\n",
            "Epoch 194/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0134 - acc: 0.0125 - val_loss: 0.0170 - val_acc: 0.0000e+00\n",
            "Epoch 195/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0133 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 196/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0133 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 197/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0131 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 198/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0136 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 199/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0136 - acc: 0.0125 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
            "Epoch 200/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0130 - acc: 0.0125 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 201/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0130 - acc: 0.0125 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 202/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0130 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 203/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0129 - acc: 0.0125 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 204/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0130 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 205/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0130 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 206/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0125 - acc: 0.0125 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
            "Epoch 207/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0134 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 208/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0128 - acc: 0.0125 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
            "Epoch 209/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0124 - acc: 0.0125 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 210/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0141 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 211/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0125 - acc: 0.0125 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
            "Epoch 212/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0128 - acc: 0.0125 - val_loss: 0.0159 - val_acc: 0.0000e+00\n",
            "Epoch 213/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0129 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 214/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0129 - acc: 0.0125 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 215/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0127 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 216/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0125 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 217/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0126 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 218/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0121 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 219/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 220/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 221/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 0.0125 - val_loss: 0.0161 - val_acc: 0.0000e+00\n",
            "Epoch 222/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0120 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 223/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0119 - acc: 0.0125 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 224/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0118 - acc: 0.0125 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 225/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0121 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 226/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0117 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 227/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0120 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 228/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0117 - acc: 0.0125 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 229/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0117 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 230/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0118 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 231/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 0.0125 - val_loss: 0.0164 - val_acc: 0.0000e+00\n",
            "Epoch 232/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0117 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 233/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0118 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 234/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 0.0125 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 235/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0119 - acc: 0.0125 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
            "Epoch 236/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0115 - acc: 0.0125 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
            "Epoch 237/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 238/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0117 - acc: 0.0125 - val_loss: 0.0149 - val_acc: 0.0000e+00\n",
            "Epoch 239/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0114 - acc: 0.0125 - val_loss: 0.0149 - val_acc: 0.0000e+00\n",
            "Epoch 240/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0113 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 241/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 0.0125 - val_loss: 0.0149 - val_acc: 0.0000e+00\n",
            "Epoch 242/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 243/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.0125 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 244/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.0125 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 245/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.0125 - val_loss: 0.0148 - val_acc: 0.0000e+00\n",
            "Epoch 246/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.0125 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 247/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0110 - acc: 0.0125 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
            "Epoch 248/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.0125 - val_loss: 0.0148 - val_acc: 0.0000e+00\n",
            "Epoch 249/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 0.0125 - val_loss: 0.0147 - val_acc: 0.0000e+00\n",
            "Epoch 250/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0111 - acc: 0.0125 - val_loss: 0.0149 - val_acc: 0.0000e+00\n",
            "Epoch 251/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.0125 - val_loss: 0.0144 - val_acc: 0.0000e+00\n",
            "Epoch 252/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.0125 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
            "Epoch 253/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 0.0125 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
            "Epoch 254/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.0125 - val_loss: 0.0143 - val_acc: 0.0000e+00\n",
            "Epoch 255/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0104 - acc: 0.0125 - val_loss: 0.0144 - val_acc: 0.0000e+00\n",
            "Epoch 256/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0104 - acc: 0.0125 - val_loss: 0.0144 - val_acc: 0.0000e+00\n",
            "Epoch 257/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.0125 - val_loss: 0.0147 - val_acc: 0.0000e+00\n",
            "Epoch 258/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0104 - acc: 0.0125 - val_loss: 0.0149 - val_acc: 0.0000e+00\n",
            "Epoch 259/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 0.0125 - val_loss: 0.0145 - val_acc: 0.0000e+00\n",
            "Epoch 260/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 261/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.0125 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 262/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 0.0125 - val_loss: 0.0141 - val_acc: 0.0000e+00\n",
            "Epoch 263/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0108 - acc: 0.0125 - val_loss: 0.0141 - val_acc: 0.0000e+00\n",
            "Epoch 264/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.0125 - val_loss: 0.0141 - val_acc: 0.0000e+00\n",
            "Epoch 265/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.0125 - val_loss: 0.0141 - val_acc: 0.0000e+00\n",
            "Epoch 266/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0101 - acc: 0.0125 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
            "Epoch 267/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.0125 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
            "Epoch 268/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0127 - acc: 0.0125 - val_loss: 0.0140 - val_acc: 0.0000e+00\n",
            "Epoch 269/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0102 - acc: 0.0125 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 270/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.0125 - val_loss: 0.0142 - val_acc: 0.0000e+00\n",
            "Epoch 271/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.0125 - val_loss: 0.0142 - val_acc: 0.0000e+00\n",
            "Epoch 272/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 0.0125 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 273/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0098 - acc: 0.0125 - val_loss: 0.0140 - val_acc: 0.0000e+00\n",
            "Epoch 274/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.0125 - val_loss: 0.0140 - val_acc: 0.0000e+00\n",
            "Epoch 275/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.0125 - val_loss: 0.0135 - val_acc: 0.0000e+00\n",
            "Epoch 276/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 0.0125 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 277/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.0125 - val_loss: 0.0134 - val_acc: 0.0000e+00\n",
            "Epoch 278/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 0.0125 - val_loss: 0.0135 - val_acc: 0.0000e+00\n",
            "Epoch 279/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.0125 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
            "Epoch 280/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.0125 - val_loss: 0.0137 - val_acc: 0.0000e+00\n",
            "Epoch 281/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.0125 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
            "Epoch 282/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.0125 - val_loss: 0.0141 - val_acc: 0.0000e+00\n",
            "Epoch 283/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 0.0125 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 284/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 0.0125 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
            "Epoch 285/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.0125 - val_loss: 0.0145 - val_acc: 0.0000e+00\n",
            "Epoch 286/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.0125 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 287/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0092 - acc: 0.0125 - val_loss: 0.0133 - val_acc: 0.0000e+00\n",
            "Epoch 288/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.0125 - val_loss: 0.0134 - val_acc: 0.0000e+00\n",
            "Epoch 289/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.0125 - val_loss: 0.0131 - val_acc: 0.0000e+00\n",
            "Epoch 290/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.0125 - val_loss: 0.0131 - val_acc: 0.0000e+00\n",
            "Epoch 291/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 0.0125 - val_loss: 0.0132 - val_acc: 0.0000e+00\n",
            "Epoch 292/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0093 - acc: 0.0125 - val_loss: 0.0134 - val_acc: 0.0000e+00\n",
            "Epoch 293/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.0125 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 294/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0087 - acc: 0.0125 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 295/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 0.0125 - val_loss: 0.0144 - val_acc: 0.0000e+00\n",
            "Epoch 296/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.0125 - val_loss: 0.0135 - val_acc: 0.0000e+00\n",
            "Epoch 297/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0088 - acc: 0.0125 - val_loss: 0.0137 - val_acc: 0.0000e+00\n",
            "Epoch 298/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0090 - acc: 0.0125 - val_loss: 0.0129 - val_acc: 0.0000e+00\n",
            "Epoch 299/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0087 - acc: 0.0125 - val_loss: 0.0130 - val_acc: 0.0000e+00\n",
            "Epoch 300/300\n",
            "80/80 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 0.0125 - val_loss: 0.0129 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoWkBT_K3P8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnw_frTh3hsm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6e6d3bb1-177f-40d0-88c2-9bb5c9316182"
      },
      "source": [
        "plt.scatter( range(20), x_pred, c='r')\n",
        "plt.scatter( range(20), y_test, c='g')\n",
        "plt.show()"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX5JKwsCXpJIw8CWpJP4/GmEXfvW8EvwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTciX5Dd38V-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "3a7e2696-02bf-4066-cb9f-ab464720f622"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8u_HsNc4S4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f421890f-6668-4449-8b2a-f50860b5a8ae"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7CEh7fxGNnc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "40794c45-e387-4bb1-8826-13d4b31c6792"
      },
      "source": [
        "plt.plot(history.history['val_acc'])\n",
        "plt.show()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGCRJHYNBktQxGCRJHYNBktQxGCRJnf8FUpB1hhOgYqwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ4-W8W-WQbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}